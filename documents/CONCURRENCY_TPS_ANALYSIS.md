# Concurrency vs TPS Analysis

## Your Observation
- **Initial**: 100 virtual users â†’ 13,300 TPS
- **Ramp-up**: 200, 300, 400... virtual users â†’ TPS stays at ~13,300

## Why This Happens (Root Cause Analysis)

### The Correct Behavior âœ…

This is **exactly correct** and reveals important performance characteristics:

#### 1. **Your HTTP Service is the Bottleneck**
```
100 users â†’ 13,300 TPS (service saturated)
200 users â†’ 13,300 TPS (service still saturated)
300 users â†’ 13,300 TPS (service still saturated)
```

**What's happening:**
- Your HTTP service can handle a maximum of ~13,300 requests/second
- Once you hit this limit, adding more virtual users doesn't increase throughput
- The extra users just wait longer for responses

#### 2. **What SHOULD Change As You Ramp Up**

| Metric | Expected Behavior |
|--------|------------------|
| **TPS** | Stays constant at ~13,300 (service limit) âœ… |
| **Active Virtual Users** | Increases: 100 â†’ 200 â†’ 300 â†’ 400... âœ… |
| **Average Latency** | **INCREASES** (more users = more competition) âš ï¸ |
| **P95/P99 Latency** | **INCREASES SIGNIFICANTLY** âš ï¸ |
| **Pending Tasks** | May increase if queue builds up |

### The Key Metric: Latency

**This is what you should watch during ramp-up:**

```
100 users:
  - TPS: 13,300
  - Avg Latency: 7.5ms  (100 users Ã· 13,300 TPS = 7.5ms per request)

200 users:
  - TPS: 13,300 (same!)
  - Avg Latency: 15ms   (200 users Ã· 13,300 TPS = 15ms per request) âš ï¸

400 users:
  - TPS: 13,300 (same!)
  - Avg Latency: 30ms   (400 users Ã· 13,300 TPS = 30ms per request) âš ï¸
```

**Why latency increases:**
- Same TPS (13,300) is now shared among more users
- Each user has to wait longer for their turn
- Queue depth increases

## Is This a Problem?

### No - This is Capacity Testing! âœ…

You've successfully discovered:
1. **Maximum throughput**: ~13,300 TPS
2. **Saturation point**: 100 concurrent users is enough to saturate the service
3. **Scalability limit**: Adding more users doesn't help - the service is the bottleneck

### What You're Testing

**Concurrency-based testing** answers:
- âœ… "How many concurrent users can the system handle?"
- âœ… "At what point does latency become unacceptable?"
- âœ… "What is the maximum throughput under load?"

**NOT**:
- âŒ "Can I get more than 13,300 TPS?" (No - service limit reached)

## Diagnosing the Bottleneck

### Is it the HTTP Service or VajraEdge?

**Quick Test:**

1. **Check pending tasks**:
   - If pending tasks = 0 â†’ VajraEdge can keep up âœ…
   - If pending tasks > 0 â†’ VajraEdge is the bottleneck âš ï¸

2. **Check latency distribution**:
   - If P50 â‰ˆ P99 â†’ Service responds consistently (good)
   - If P99 >> P50 â†’ Service is struggling (queue buildup)

3. **Run simple load test**:
   ```bash
   # Test with different concurrency levels
   100 users â†’ 13,300 TPS
   50 users  â†’ ??? TPS (should be ~6,650 if linear)
   10 users  â†’ ??? TPS (should be ~1,330 if linear)
   ```

### Expected Results

If the service is the bottleneck (most likely):
```
10 users  â†’ ~1,300 TPS   (linear relationship)
50 users  â†’ ~6,500 TPS   (linear relationship)
100 users â†’ 13,300 TPS   (saturated!)
200 users â†’ 13,300 TPS   (saturated!)
```

If VajraEdge is the bottleneck (unlikely):
```
10 users  â†’ ~1,300 TPS
50 users  â†’ ~6,500 TPS
100 users â†’ ~10,000 TPS  (VajraEdge maxed out)
200 users â†’ ~10,000 TPS  (VajraEdge maxed out)
Pending tasks would increase
```

## What To Look For

### Healthy Load Test (Service is Bottleneck)
```
Virtual Users: 100 â†’ 200 â†’ 300
TPS:          13.3K â†’ 13.3K â†’ 13.3K  âœ… (constant - service limit)
Active Tasks: 100 â†’ 200 â†’ 300       âœ… (increases linearly)
Pending:      0 â†’ 0 â†’ 0              âœ… (VajraEdge keeping up)
Avg Latency:  7ms â†’ 15ms â†’ 30ms     âš ï¸ (increases - expected!)
P99 Latency:  20ms â†’ 50ms â†’ 100ms   âš ï¸ (increases more - queue buildup)
```

### Unhealthy (VajraEdge is Bottleneck)
```
Virtual Users: 100 â†’ 200 â†’ 300
TPS:          10K â†’ 10K â†’ 10K        âš ï¸ (capped below service capacity)
Active Tasks: 100 â†’ 150 â†’ 150       âš ï¸ (not increasing linearly)
Pending:      0 â†’ 50 â†’ 150           ğŸ”´ (queue building up)
```

## Recommendations

### 1. Verify Active Tasks Increase Linearly
**Expected:**
- 100 users â†’ 100 active tasks
- 200 users â†’ 200 active tasks
- 300 users â†’ 300 active tasks

**If not**, there's a concurrency limiting issue.

### 2. Monitor Latency Trends
- Watch **P95 and P99** latency during ramp-up
- These should increase as more users compete
- **Sudden spikes** indicate queue saturation

### 3. Find Optimal Concurrency
The "sweet spot" is where:
- TPS is maximized (~13,300)
- Latency is acceptable (< 100ms?)
- Error rate is low (< 1%)

For your service, this might be:
- **100 users**: Max throughput, reasonable latency
- **200 users**: Same throughput, higher latency (wasteful)

### 4. Improve the HTTP Service (If Needed)
If 13,300 TPS isn't enough:
- Scale horizontally (more instances)
- Optimize the service code
- Add caching
- Use connection pooling
- Optimize database queries

## Conclusion

**Your observations are correct!** 

The TPS staying constant at 13,300 while ramping up virtual users is **expected behavior** when the HTTP service is the bottleneck. This is exactly what concurrency-based load testing is designed to reveal.

The key metrics to watch are:
1. âœ… **TPS** - shows maximum throughput
2. âœ… **Active Tasks** - should increase with ramp-up
3. âš ï¸ **Latency** - shows degradation under load
4. âš ï¸ **Pending Tasks** - shows if VajraEdge is keeping up

You've successfully stress-tested the system and found its limits! ğŸ‰
